{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNskna3HpBIwTgSppsD952V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dicad/parallel_programming_CuTonala_1_23/blob/BFS%2C-DFS%2C-MPI-and-CUDA/BFS_and_DFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BFS and DFS with MPI and CUDA**"
      ],
      "metadata": {
        "id": "Ym7i2jlVaLm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py\n",
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wqw19ETtbBOm",
        "outputId": "1dbac801-bdc2-48eb-e595-76b3999d6694"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2023.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2023.1.1-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.3)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2023.1-cp310-cp310-linux_x86_64.whl size=661263 sha256=83811be4af132de3dcb25d12317ee25cb8096b34ea4fd2524f4641e6b51f096e\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/65/06/b997165edd2fd9690c3497ca54ea4485b571d7bd959c21c6c4\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.0 pycuda-2023.1 pytools-2023.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from mpi4py import MPI\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.gpuarray as gpuarray\n",
        "import pycuda.compiler\n",
        "\n",
        "cuda_code_bfs = \"\"\"\n",
        "__global__ void bfs_kernel(int *graph, int *queue, int *visited, int *result, int size) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    while (tid < size) {\n",
        "        int current_node = queue[tid];\n",
        "        if (!visited[current_node]) {\n",
        "            visited[current_node] = 1;\n",
        "            result[current_node] = 1;\n",
        "\n",
        "            for (int neighbor = 0; neighbor < size; ++neighbor) {\n",
        "                if (graph[current_node * size + neighbor] && !visited[neighbor]) {\n",
        "                    queue[size + tid] = neighbor;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "cuda_code_dfs = \"\"\"\n",
        "__global__ void dfs_kernel(int *graph, int *stack, int *visited, int *result, int size) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    while (tid < size) {\n",
        "        int current_node = stack[tid];\n",
        "        if (!visited[current_node]) {\n",
        "            visited[current_node] = 1;\n",
        "            result[current_node] = 1;\n",
        "\n",
        "            for (int neighbor = 0; neighbor < size; ++neighbor) {\n",
        "                if (graph[current_node * size + neighbor] && !visited[neighbor]) {\n",
        "                    stack[size + tid] = neighbor;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        tid += blockDim.x * gridDim.x;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize CUDA context\n",
        "cuda.init()\n",
        "dev = cuda.Device(0)  # Adjust the device index as needed\n",
        "ctx = dev.make_context()\n",
        "\n",
        "try:\n",
        "    # Compile CUDA code\n",
        "    cuda_module_bfs = pycuda.compiler.SourceModule(cuda_code_bfs)\n",
        "    cuda_module_dfs = pycuda.compiler.SourceModule(cuda_code_dfs)\n",
        "\n",
        "    # Get CUDA kernel functions\n",
        "    bfs_kernel = cuda_module_bfs.get_function(\"bfs_kernel\")\n",
        "    dfs_kernel = cuda_module_dfs.get_function(\"dfs_kernel\")\n",
        "\n",
        "    def partition_graph_gpu(graph, num_partitions):\n",
        "        partitions = [np.zeros((len(graph),), dtype=np.int32) for _ in range(num_partitions)]\n",
        "\n",
        "        for i, node_key in enumerate(graph.keys()):\n",
        "            partition_id = i % num_partitions\n",
        "            partitions[partition_id][i] = 1  # Represent the presence of the node in the partition\n",
        "\n",
        "        return partitions\n",
        "\n",
        "    def bfs_gpu(graph, start_node, result_gpu):\n",
        "        visited_gpu = gpuarray.zeros(len(graph), dtype=np.int32)\n",
        "        queue_gpu = gpuarray.to_gpu(np.array([start_node], dtype=np.int32))\n",
        "\n",
        "        block_size = 128\n",
        "        grid_size = (len(graph) + block_size - 1) // block_size\n",
        "\n",
        "        bfs_kernel(graph, queue_gpu, visited_gpu, result_gpu, np.int32(len(graph)),\n",
        "                   block=(block_size, 1, 1), grid=(grid_size, 1))\n",
        "\n",
        "    def dfs_gpu(graph, start_node, result_gpu):\n",
        "        visited_gpu = gpuarray.zeros(len(graph), dtype=np.int32)\n",
        "        stack_gpu = gpuarray.to_gpu(np.array([start_node], dtype=np.int32))\n",
        "\n",
        "        block_size = 128\n",
        "        grid_size = (len(graph) + block_size - 1) // block_size\n",
        "\n",
        "        dfs_kernel(graph, stack_gpu, visited_gpu, result_gpu, np.int32(len(graph)),\n",
        "                   block=(block_size, 1, 1), grid=(grid_size, 1))\n",
        "\n",
        "    comm = MPI.COMM_WORLD\n",
        "    rank = comm.Get_rank()\n",
        "    size = comm.Get_size()\n",
        "    graph = {\n",
        "        0: [1, 2],\n",
        "        1: [0, 3, 4],\n",
        "        2: [0, 5],\n",
        "        3: [1, 5],\n",
        "        4: [1, 5],\n",
        "        5: [2, 3, 4]\n",
        "    }\n",
        "\n",
        "    # Divide the graph among MPI processes\n",
        "    graph_partitions = partition_graph_gpu(graph, size)\n",
        "    local_partition = graph_partitions[rank]\n",
        "\n",
        "    # Perform BFS or DFS on each partition using GPU\n",
        "    result_gpu = gpuarray.zeros(len(graph), dtype=np.int32)\n",
        "\n",
        "    if rank == 0:\n",
        "        # Root process handles overall coordination and final result\n",
        "        partial_results_gpu = [gpuarray.to_gpu(np.array(local_partition, dtype=np.int32))]\n",
        "        for i in range(1, size):\n",
        "            partial_results_gpu.append(comm.recv(source=i))\n",
        "\n",
        "        for partial_result_gpu in partial_results_gpu:\n",
        "            bfs_gpu(partial_result_gpu, 0, result_gpu)\n",
        "\n",
        "        result_cpu = result_gpu.get()\n",
        "        print(\"Final result:\", set(np.where(result_cpu == 1)[0]))\n",
        "    else:\n",
        "        # Worker processes perform BFS or DFS on their respective partitions using GPU\n",
        "        bfs_gpu(local_partition, rank, result_gpu)\n",
        "        comm.send(result_gpu, dest=0)\n",
        "\n",
        "finally:\n",
        "    # Ensure to clean up the CUDA context\n",
        "    ctx.pop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3vhw3oIdo4K",
        "outputId": "4a576f89-7572-4b74-a079-c5f48f5db3dc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: module in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final result: {0}\n"
          ]
        }
      ]
    }
  ]
}